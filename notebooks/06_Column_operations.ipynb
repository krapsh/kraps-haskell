{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column-based functions\n",
    "\n",
    "When building a pipeline, it is often the case that the top-level dataframe is complex and changing, but that the work focus on transforming _columns_ of this dataframe. Karps provides a way to express and compose complex functions on columns without having to run computations. Behind the scenes, Karps is able to take these functions and translate them into sequences of queries without having to deal with the details of collecting, joining and broadcasting data.\n",
    "\n",
    "As an example, we are going to build functions that take a numerical dataset and that produce a _centered_ version (the mean is substracted) and a _scaled_ version (the variance is scaled to 1). Of course, such basic\n",
    "operations are already built into Spark, but it is instructive to see how one would implement similar \n",
    "functions in practice.\n",
    "\n",
    "We will see that thanks to laziness and determinism, Karps is able to reuse some computations, and provide a high-level, lazy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ":load KarpsDisplays KarpsDagDisplay\n",
    ":extension DeriveGeneric\n",
    ":extension FlexibleContexts\n",
    ":extension OverloadedStrings\n",
    ":extension GeneralizedNewtypeDeriving\n",
    ":extension FlexibleInstances\n",
    ":extension MultiParamTypeClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Spark.Core.Dataset\n",
    "import Spark.Core.Context\n",
    "import Spark.Core.Column\n",
    "import Spark.Core.ColumnFunctions\n",
    "import Spark.Core.Functions\n",
    "import Spark.Core.Row\n",
    "import Spark.Core.Types\n",
    "import Spark.Core.Try\n",
    "\n",
    "import qualified Data.Vector as V\n",
    "import qualified Data.Text as T\n",
    "import Data.Text(Text)\n",
    "import GHC.Generics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with an extremely simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let ds = dataset [-1, 1] :: Dataset Int\n",
    "-- A column of data containing integers\n",
    "let myData = asCol ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a first function that computes the mean of the data in a column.\n",
    "Giving some names to the elements is not necessary but helps when looking at the DAG of computations.\n",
    "\n",
    "Note that we can use all the usual operators (+, /, etc.) even if the computation is lazy.\n",
    "\n",
    "Also, note that all the operations are strongly typed: unlike SQL, the casting is almost always explicit\n",
    "since it can lead to loss of precision (or worse) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myMean :: Column ref Int -> LocalData Double\n",
    "myMean col =\n",
    "  let\n",
    "    cnt = asDouble (countCol col) @@ \"mean_count\"\n",
    "    s = asDouble (sumCol col) @@ \"mean_sum\"\n",
    "  in (s / cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we apply it to our data, the result is rather anti-climactic: we just get a `LocalData` out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/localdiv_815e11@org.spark.LocalDiv!double"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myMean myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build on this to make the centering function, which simply substracts the mean, and the scaling function, which builds on the other two:\n",
    "\n",
    "Note that again, we need to cast the column, it is not going to be done for us.\n",
    "\n",
    "Note: due a Haskell limitation, the `-` operation is replaced by a `.-`. This is because Haskell does not allow to mix different types together (here a column and an observable). This restriction is going to be lifted in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myCenter :: Column ref Int -> Column ref Double\n",
    "myCenter col =\n",
    "  let m = (myMean col) @@ \"center_mean\"\n",
    "  in (asDoubleCol col) .- m\n",
    "\n",
    "myScaler :: Column ref Int -> Column ref Double\n",
    "myScaler col =\n",
    "  let cnt = asDouble (countCol col) @@ \"count\"\n",
    "      centered = myCenter col\n",
    "      stdDev = sumCol (centered * centered) / cnt @@ \"std_dev\"\n",
    "  in centered ./ stdDev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the transform look like if we apply it? Let's run `showGraph` on our simple dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Styles used for the Hoogle display in the pager */\n",
       ".hoogle-doc {\n",
       "display: block;\n",
       "padding-bottom: 1.3em;\n",
       "padding-left: 0.4em;\n",
       "}\n",
       ".hoogle-code {\n",
       "display: block;\n",
       "font-family: monospace;\n",
       "white-space: pre;\n",
       "}\n",
       ".hoogle-text {\n",
       "display: block;\n",
       "}\n",
       ".hoogle-name {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-head {\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-sub {\n",
       "display: block;\n",
       "margin-left: 0.4em;\n",
       "}\n",
       ".hoogle-package {\n",
       "font-weight: bold;\n",
       "font-style: italic;\n",
       "}\n",
       ".hoogle-module {\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-class {\n",
       "font-weight: bold;\n",
       "}\n",
       ".get-type {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "font-family: monospace;\n",
       "display: block;\n",
       "white-space: pre-wrap;\n",
       "}\n",
       ".show-type {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "font-family: monospace;\n",
       "margin-left: 1em;\n",
       "}\n",
       ".mono {\n",
       "font-family: monospace;\n",
       "display: block;\n",
       "}\n",
       ".err-msg {\n",
       "color: red;\n",
       "font-style: italic;\n",
       "font-family: monospace;\n",
       "white-space: pre;\n",
       "display: block;\n",
       "}\n",
       "#unshowable {\n",
       "color: red;\n",
       "font-weight: bold;\n",
       "}\n",
       ".err-msg.in.collapse {\n",
       "padding-top: 0.7em;\n",
       "}\n",
       ".highlight-code {\n",
       "white-space: pre;\n",
       "font-family: monospace;\n",
       "}\n",
       ".suggestion-warning { \n",
       "font-weight: bold;\n",
       "color: rgb(200, 130, 0);\n",
       "}\n",
       ".suggestion-error { \n",
       "font-weight: bold;\n",
       "color: red;\n",
       "}\n",
       ".suggestion-name {\n",
       "font-weight: bold;\n",
       "}\n",
       "</style><iframe seamless style='width:900px;height:620px;border:0' srcdoc='<script>function mycleanup1() {var docs = document.getElementsByClassName(\"side tf-graph-basic\");var n = docs.length;for (var i=0; i < n; i++) {var x = docs[i];if (x.style.display === \"\") {x.style.display = \"none\";}}};function mycleanup2() {var docs = document.getElementsByClassName(\"main tf-graph-basic\");var n = docs.length;for (var i=0; i < n; i++) {var x = docs[i];if (x.style.left !== 0) {x.style.left = 0;}}};function load() {{  document.getElementById(\"6716995761877399895\").pbtxt = \"node {\\n  name: \\\"distributedliteral_e9167d\\\"\\n  op: \\\"org.spark.DistributedLiteral\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"e9167..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"distributed\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"int\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa\\\"\\n  op: \\\"org.spark.StructuredReduction\\\"\\n  input: \\\"distributedliteral_e9167d\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"148aa..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"int\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"mean_sum\\\"\\n  op: \\\"org.spark.Select\\\"\\n  input: \\\"sum_148aaa\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"181af..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"count_624031\\\"\\n  op: \\\"org.spark.StructuredReduction\\\"\\n  input: \\\"distributedliteral_e9167d\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"62403..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"int\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"mean_count\\\"\\n  op: \\\"org.spark.Select\\\"\\n  input: \\\"count_624031\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"37bf0..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"center_mean\\\"\\n  op: \\\"org.spark.LocalDiv\\\"\\n  input: \\\"mean_sum\\\"\\n  input: \\\"mean_count\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"815e1..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"localpack_12d6b8\\\"\\n  op: \\\"org.spark.LocalPack\\\"\\n  input: \\\"center_mean\\\"\\n  input: \\\"center_mean\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"12d6b..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"{_1:double _2:double}\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"broadcastpair_c502ed\\\"\\n  op: \\\"org.spark.BroadcastPair\\\"\\n  input: \\\"distributedliteral_e9167d\\\"\\n  input: \\\"localpack_12d6b8\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"c502e..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"distributed\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"{_1:int _2:{_1:double _2:double}}\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"select_4800d7\\\"\\n  op: \\\"org.spark.Select\\\"\\n  input: \\\"broadcastpair_c502ed\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"4800d..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"distributed\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517\\\"\\n  op: \\\"org.spark.StructuredReduction\\\"\\n  input: \\\"select_4800d7\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"9c551..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"std_dev\\\"\\n  op: \\\"org.spark.LocalDiv\\\"\\n  input: \\\"sum_9c5517\\\"\\n  input: \\\"mean_count\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"8e931..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"localpack_bb6144\\\"\\n  op: \\\"org.spark.LocalPack\\\"\\n  input: \\\"center_mean\\\"\\n  input: \\\"std_dev\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"bb614..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"local\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"{_1:double _2:double}\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"broadcastpair_59ae8f\\\"\\n  op: \\\"org.spark.BroadcastPair\\\"\\n  input: \\\"distributedliteral_e9167d\\\"\\n  input: \\\"localpack_bb6144\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"59ae8..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"distributed\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"{_1:int _2:{_1:double _2:double}}\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"select_c905de\\\"\\n  op: \\\"org.spark.Select\\\"\\n  input: \\\"broadcastpair_59ae8f\\\"\\n  attr {\\n    key: \\\"id\\\"\\n    value {\\n      val: \\\"c905d..\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"locality\\\"\\n    value {\\n      val: \\\"distributed\\\"\\n    }\\n  }\\n  attr {\\n    key: \\\"sqlType\\\"\\n    value {\\n      val: \\\"double\\\"\\n    }\\n  }\\n}\\n\";  setInterval(mycleanup1, 500);  setInterval(mycleanup2, 500);}}</script><link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load() onscroll=scroll()><div style=\"height:600px\"><tf-graph-basic id=\"6716995761877399895\"></tf-graph-basic></div>'></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- make a new scaled column:\n",
    "let scaled = myScaler myData\n",
    "-- pack it into a dataset to visualize it:\n",
    "let out = pack1 scaled\n",
    "showGraph out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This graph is pretty complicated, and you should click around to see what each node corresponds to.\n",
    "A couple of points are noteworthy:\n",
    "\n",
    " - Karps handles automatically and seemlessly the broadcasting and the reduction of the variables.\n",
    "   In fact, Karps can broadcast pretty much anything that is understood by Spark dataframes.\n",
    "\n",
    " - Karps tries to reuse computations as much as possible: even if we did not make any attempt for it,\n",
    "   the count of the dataset is reused between the calculation of the mean and of the variance.\n",
    "   This is only possible because of laziness.\n",
    "   \n",
    " - thanks to naming, even if the functions happen to be nested, we can still quickly relate one\n",
    "   operator to the function that generated it.\n",
    "\n",
    "Now, let's execute all of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Debug] Creating spark session at url: http://10.0.2.2:8081/sessions/col_ops6 @(<unknown>:<unknown> <unknown>:0:0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = defaultConf {\n",
    "        confEndPoint = \"http://10.0.2.2\",\n",
    "        confRequestedSessionName = \"col_ops6\" }\n",
    "\n",
    "createSparkSessionDef conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Debug] executeCommand1': computing observable /collect_f09694@org.spark.Collect![double] @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] Sending computations at url: http://10.0.2.2:8081/computations/col_ops6/0/createwith nodes: [/distributedliteral_e9167d@org.spark.DistributedLiteral:int,/sum_148aaa@SUM!int,/mean_sum@org.spark.Select!double,/count_624031@COUNT!int,/mean_count@org.spark.Select!double,/center_mean@org.spark.LocalDiv!double,/localpack_12d6b8@org.spark.LocalPack!{_1:double _2:double},/broadcastpair_c502ed@org.spark.BroadcastPair:{_1:int _2:{_1:double _2:double}},/select_4800d7@org.spark.Select:double,/sum_9c5517@SUM!double,/std_dev@org.spark.LocalDiv!double,/localpack_bb6144@org.spark.LocalPack!{_1:double _2:double},/broadcastpair_59ae8f@org.spark.BroadcastPair:{_1:int _2:{_1:double _2:double}},/select_c905de@org.spark.Select:double,/collect_f09694@org.spark.Collect![double]] @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /sum_148aaa running @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /count_624031 running @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /sum_148aaa finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /mean_sum finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /count_624031 finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /mean_count finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /center_mean finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /localpack_12d6b8 finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /sum_9c5517 running @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /std_dev running @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /localpack_bb6144 finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /collect_f09694 running @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /sum_9c5517 finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /std_dev finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[Info] _computationMultiStatus: /collect_f09694 finished @(<unknown>:<unknown> <unknown>:0:0)\n",
       "[-1.0,1.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exec1Def (collect scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preview of the next chapter, here is the function to display the RDDs generated by Spark when running this command.\n",
    "\n",
    "Each element comes from the graph before. You can see which ones are missing (they have been optimized away\n",
    "by Spark). When you click on a box, you can see the sequence of RDDs that was generated in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Styles used for the Hoogle display in the pager */\n",
       ".hoogle-doc {\n",
       "display: block;\n",
       "padding-bottom: 1.3em;\n",
       "padding-left: 0.4em;\n",
       "}\n",
       ".hoogle-code {\n",
       "display: block;\n",
       "font-family: monospace;\n",
       "white-space: pre;\n",
       "}\n",
       ".hoogle-text {\n",
       "display: block;\n",
       "}\n",
       ".hoogle-name {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-head {\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-sub {\n",
       "display: block;\n",
       "margin-left: 0.4em;\n",
       "}\n",
       ".hoogle-package {\n",
       "font-weight: bold;\n",
       "font-style: italic;\n",
       "}\n",
       ".hoogle-module {\n",
       "font-weight: bold;\n",
       "}\n",
       ".hoogle-class {\n",
       "font-weight: bold;\n",
       "}\n",
       ".get-type {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "font-family: monospace;\n",
       "display: block;\n",
       "white-space: pre-wrap;\n",
       "}\n",
       ".show-type {\n",
       "color: green;\n",
       "font-weight: bold;\n",
       "font-family: monospace;\n",
       "margin-left: 1em;\n",
       "}\n",
       ".mono {\n",
       "font-family: monospace;\n",
       "display: block;\n",
       "}\n",
       ".err-msg {\n",
       "color: red;\n",
       "font-style: italic;\n",
       "font-family: monospace;\n",
       "white-space: pre;\n",
       "display: block;\n",
       "}\n",
       "#unshowable {\n",
       "color: red;\n",
       "font-weight: bold;\n",
       "}\n",
       ".err-msg.in.collapse {\n",
       "padding-top: 0.7em;\n",
       "}\n",
       ".highlight-code {\n",
       "white-space: pre;\n",
       "font-family: monospace;\n",
       "}\n",
       ".suggestion-warning { \n",
       "font-weight: bold;\n",
       "color: rgb(200, 130, 0);\n",
       "}\n",
       ".suggestion-error { \n",
       "font-weight: bold;\n",
       "color: red;\n",
       "}\n",
       ".suggestion-name {\n",
       "font-weight: bold;\n",
       "}\n",
       "</style><iframe seamless style='width:900px;height:620px;border:0' srcdoc='<script>function mycleanup1() {var docs = document.getElementsByClassName(\"side tf-graph-basic\");var n = docs.length;for (var i=0; i < n; i++) {var x = docs[i];if (x.style.display === \"\") {x.style.display = \"none\";}}};function mycleanup2() {var docs = document.getElementsByClassName(\"main tf-graph-basic\");var n = docs.length;for (var i=0; i < n; i++) {var x = docs[i];if (x.style.left !== 0) {x.style.left = 0;}}};function load() {{  document.getElementById(\"6018053928128288163\").pbtxt = \"node {\\n  name: \\\"count_624031/ParallelCollectionRDD-8\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[8] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"count_624031/MapPartitionsRDD-9\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"count_624031/ParallelCollectionRDD-8\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[9] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"count_624031/MapPartitionsRDD-10\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"count_624031/MapPartitionsRDD-9\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[10] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"count_624031/MapPartitionsRDD-11\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"count_624031/MapPartitionsRDD-10\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[11] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"count_624031/ShuffledRowRDD-12\\\"\\n  op: \\\"ShuffledRowRDD\\\"\\n  input: \\\"count_624031/MapPartitionsRDD-11\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ShuffledRowRDD[12] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"count_624031/MapPartitionsRDD-13\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"count_624031/ShuffledRowRDD-12\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[13] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa/ParallelCollectionRDD-2\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[2] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa/MapPartitionsRDD-3\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_148aaa/ParallelCollectionRDD-2\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[3] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa/MapPartitionsRDD-4\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_148aaa/MapPartitionsRDD-3\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[4] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa/MapPartitionsRDD-5\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_148aaa/MapPartitionsRDD-4\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[5] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa/ShuffledRowRDD-6\\\"\\n  op: \\\"ShuffledRowRDD\\\"\\n  input: \\\"sum_148aaa/MapPartitionsRDD-5\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ShuffledRowRDD[6] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_148aaa/MapPartitionsRDD-7\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_148aaa/ShuffledRowRDD-6\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[7] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"center_mean/ParallelCollectionRDD-18\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^mean_count/MapPartitionsRDD-17:0\\\"\\n  input: \\\"^mean_sum/MapPartitionsRDD-15:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[18] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"center_mean/MapPartitionsRDD-19\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"center_mean/ParallelCollectionRDD-18\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[19] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"localpack_12d6b8/ParallelCollectionRDD-20\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^center_mean/MapPartitionsRDD-19:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[20] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"localpack_12d6b8/MapPartitionsRDD-21\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"localpack_12d6b8/ParallelCollectionRDD-20\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[21] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"mean_sum/ParallelCollectionRDD-14\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^sum_148aaa/MapPartitionsRDD-7:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[14] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"mean_sum/MapPartitionsRDD-15\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"mean_sum/ParallelCollectionRDD-14\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[15] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"localpack_bb6144/ParallelCollectionRDD-34\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^center_mean/MapPartitionsRDD-19:0\\\"\\n  input: \\\"^std_dev/MapPartitionsRDD-33:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[34] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"localpack_bb6144/MapPartitionsRDD-35\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"localpack_bb6144/ParallelCollectionRDD-34\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[35] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517/ParallelCollectionRDD-26\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^localpack_12d6b8/MapPartitionsRDD-21:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[26] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517/MapPartitionsRDD-27\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_9c5517/ParallelCollectionRDD-26\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[27] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517/MapPartitionsRDD-28\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_9c5517/MapPartitionsRDD-27\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[28] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517/MapPartitionsRDD-29\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_9c5517/MapPartitionsRDD-28\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[29] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517/ShuffledRowRDD-30\\\"\\n  op: \\\"ShuffledRowRDD\\\"\\n  input: \\\"sum_9c5517/MapPartitionsRDD-29\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ShuffledRowRDD[30] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"sum_9c5517/MapPartitionsRDD-31\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"sum_9c5517/ShuffledRowRDD-30\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[31] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"collect_f09694/ParallelCollectionRDD-40\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^localpack_bb6144/MapPartitionsRDD-35:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[40] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"collect_f09694/MapPartitionsRDD-41\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"collect_f09694/ParallelCollectionRDD-40\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[41] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"collect_f09694/MapPartitionsRDD-42\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"collect_f09694/MapPartitionsRDD-41\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[42] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"collect_f09694/ShuffledRowRDD-43\\\"\\n  op: \\\"ShuffledRowRDD\\\"\\n  input: \\\"collect_f09694/MapPartitionsRDD-42\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ShuffledRowRDD[43] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"collect_f09694/MapPartitionsRDD-44\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"collect_f09694/ShuffledRowRDD-43\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[44] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"collect_f09694/MapPartitionsRDD-45\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"collect_f09694/MapPartitionsRDD-44\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[45] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"mean_count/ParallelCollectionRDD-16\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^count_624031/MapPartitionsRDD-13:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[16] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"mean_count/MapPartitionsRDD-17\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"mean_count/ParallelCollectionRDD-16\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[17] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"std_dev/ParallelCollectionRDD-32\\\"\\n  op: \\\"ParallelCollectionRDD\\\"\\n  input: \\\"^mean_count/MapPartitionsRDD-17:0\\\"\\n  input: \\\"^sum_9c5517/MapPartitionsRDD-31:0\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"ParallelCollectionRDD[32] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\nnode {\\n  name: \\\"std_dev/MapPartitionsRDD-33\\\"\\n  op: \\\"MapPartitionsRDD\\\"\\n  input: \\\"std_dev/ParallelCollectionRDD-32\\\"\\n  attr {\\n    key: \\\"name\\\"\\n    value {\\n      val: \\\"MapPartitionsRDD[33] at execute at ExecutionItem.scala:122\\\"\\n    }\\n  }\\n}\\n\";  setInterval(mycleanup1, 500);  setInterval(mycleanup2, 500);}}</script><link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load() onscroll=scroll()><div style=\"height:600px\"><tf-graph-basic id=\"6018053928128288163\"></tf-graph-basic></div>'></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRDD \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "version": "7.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
